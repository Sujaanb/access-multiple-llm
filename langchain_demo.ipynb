{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangChain Basics Demo\n\nThis notebook provides a beginner-friendly introduction to using [LangChain](https://python.langchain.com/) for interacting with Large Language Models (LLMs).\n\nWe will show how to set up the environment, create a simple prompt, and invoke an LLM using the helper utilities from this repository."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Installation\n\nMake sure you have the dependencies installed. If you're running this notebook for the first time, uncomment and run the cell below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install -r requirements.txt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setting up API keys\n\nCreate a `.env` file in the project root or set the environment variables directly in the notebook. At minimum you need to specify the `llm_provider` and its corresponding API key. Below is an example using environment variables."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['llm_provider'] = 'mistral'  # 'mistral', 'gemini', or 'openai'\nos.environ['mistral_api_key'] = 'YOUR_MISTRAL_API_KEY'\n# os.environ['gemini_api_key'] = 'YOUR_GEMINI_API_KEY'\n# os.environ['openai_api_key'] = 'YOUR_OPENAI_API_KEY'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using `LLMFactory`\n\nThe `LLMFactory` class from this repository chooses the correct LangChain wrapper based on the provider. We'll use it to generate a short summary about any topic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from util.llm_factory import LLMFactory\nfrom util.system_prompt import prompt_generate_summary\n\ntext = 'Explain the importance of clean energy in a few sentences.'\nresponse = LLMFactory.invoke(\n    system_prompt=prompt_generate_summary,\n    human_message=text,\n    temperature=0.7,\n)\nprint(response.content.strip())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Basic LangChain Components\n\nBehind the scenes, `LLMFactory.invoke` creates a prompt using `ChatPromptTemplate` and sends it to the selected model. Below is a minimal example without the factory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Use your preferred model wrapper\nchat = ChatOpenAI(api_key=os.environ['mistral_api_key'], model='gpt-3.5-turbo', temperature=0.7)\n\nsystem_tmpl = SystemMessagePromptTemplate.from_template('You are a helpful assistant.')\nhuman_tmpl = HumanMessagePromptTemplate.from_template('What is LangChain?')\n\nprompt = ChatPromptTemplate.from_messages([system_tmpl, human_tmpl])\nmessages = prompt.format_messages()\nresponse = chat.invoke(messages)\nprint(response.content.strip())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This demonstrates the basic building blocks: prompt templates, LLM wrappers, and calling `invoke`. You can extend this with chains, agents, or memory for more advanced workflows."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Building a Simple Chain\n\nLet's create a tiny `LLMChain` that takes a topic and returns a single sentence description."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Template with a variable placeholder\npt = PromptTemplate.from_template('Describe {topic} in one sentence.')\nchain = LLMChain(prompt=pt, llm=LLMFactory.create_llm_instance())\n\nres = chain.invoke({'topic': 'quantum computing'})\nprint(res['text'].strip())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conversational Memory\n\nLangChain makes it easy to store chat history. Below we use `ConversationBufferMemory` to remember what we talked about."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nchat_chain = ConversationChain(llm=LLMFactory.create_llm_instance(), memory=memory)\n\nprint(chat_chain.invoke('Hello there!')['response'])\nprint(chat_chain.invoke('What did I just say?')['response'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nThis notebook introduced basic LangChain usage: installing dependencies, configuring API keys, invoking an LLM, building prompts, creating chains, and managing conversation history."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
